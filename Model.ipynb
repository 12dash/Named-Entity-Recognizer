{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Word Level/Sentence Tag idx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_file(path):\n",
    "    file = open(path,'rb')\n",
    "    a,b = pickle.load(file)\n",
    "    file.close()\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word,word2idx =fetch_file('Word Level/word_map.pkl')\n",
    "idx2tag,tag2idx = fetch_file('Word Level/tag_map.pkl')\n",
    "df['Word idx'] =df['Word idx'].apply(lambda x: ast.literal_eval(x))\n",
    "df['Tag idx'] =df['Tag idx'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Word idx'].to_list()\n",
    "Y = df['Tag idx'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d,X_test_d, y_train_d,y_test_d = train_test_split(X,Y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sen_len = max([len(i) for i in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = word2idx['PAD']*np.ones((len(X_train_d),max_sen_len))\n",
    "y_train = -1*np.ones((len(y_train_d), max_sen_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(X_train_d)):\n",
    "    sen_len = len(X_train_d[j])\n",
    "    temp_x = np.array(X_train_d[j])\n",
    "    temp_y = np.array(y_train_d[j])\n",
    "    X_train[j][:sen_len] = (temp_x)\n",
    "    y_train[j][:sen_len] = (temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = torch.LongTensor(X_train), torch.LongTensor(y_train)\n",
    "X_train, y_train = Variable(X_train), Variable(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,lstm_hidden_dim,number_of_tags):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first = True)\n",
    "        self.fc= nn.Linear(lstm_hidden_dim, number_of_tags)\n",
    "    \n",
    "    def forward(self, s):\n",
    "        s = self.embedding(s)\n",
    "        s, _ = self.lstm(s)\n",
    "        s = s.view(-1, s.shape[2])\n",
    "        s = self.fc(s)\n",
    "        \n",
    "        return F.log_softmax(s, dim = 1)\n",
    "    \n",
    "    def loss_fn(self,outputs, labels):\n",
    "        labels = labels.view(-1)\n",
    "        mask = (labels >= 0).float()\n",
    "        #num_tokens = int(torch.sum(mask).data[0])\n",
    "        outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "        \n",
    "        return -torch.sum(outputs)/17          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "embedding_dim = max_sen_len\n",
    "lstm_hidden_dim = 64\n",
    "num_tags = len(tag2idx)\n",
    "model = Net(vocab_size = vocab_size + 1, embedding_dim = embedding_dim, lstm_hidden_dim = lstm_hidden_dim, number_of_tags = num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dataset = []\n",
    "for i in range(len(X_train)):\n",
    "    data_dataset.append((X_train[i],y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train = torch.utils.data.DataLoader(data_dataset, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.189\n",
      "[1,  4000] loss: 0.923\n",
      "[1,  6000] loss: 0.877\n",
      "[1,  8000] loss: 0.831\n",
      "[1, 10000] loss: 0.779\n",
      "[1, 12000] loss: 0.741\n",
      "[1, 14000] loss: 0.703\n",
      "[1, 16000] loss: 0.664\n",
      "[1, 18000] loss: 0.647\n",
      "[1, 20000] loss: 0.632\n",
      "[1, 22000] loss: 0.609\n",
      "[1, 24000] loss: 0.587\n",
      "[1, 26000] loss: 0.563\n",
      "[1, 28000] loss: 0.543\n",
      "[1, 30000] loss: 0.530\n",
      "[1, 32000] loss: 0.525\n",
      "[2,  2000] loss: 0.498\n",
      "[2,  4000] loss: 0.489\n",
      "[2,  6000] loss: 0.478\n",
      "[2,  8000] loss: 0.479\n",
      "[2, 10000] loss: 0.470\n",
      "[2, 12000] loss: 0.453\n",
      "[2, 14000] loss: 0.438\n",
      "[2, 16000] loss: 0.444\n",
      "[2, 18000] loss: 0.433\n",
      "[2, 20000] loss: 0.433\n",
      "[2, 22000] loss: 0.390\n",
      "[2, 24000] loss: 0.408\n",
      "[2, 26000] loss: 0.417\n",
      "[2, 28000] loss: 0.410\n",
      "[2, 30000] loss: 0.410\n",
      "[2, 32000] loss: 0.398\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(Data_train):\n",
    "        sent, labels = data\n",
    "        optimizer.zero_grad()        \n",
    "        \n",
    "        output = model(sent)\n",
    "        loss = model.loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'Word Level/model.pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
